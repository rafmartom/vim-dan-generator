#!/bin/bash

# DECLARING VARIABLES AND PROCESSING ARGS
# -------------------------------------
# (do not touch)
CURRENT_DIR="$( cd "$( dirname "${BASH_SOURCE[0]}" )" && pwd )"
source "$CURRENT_DIR/../scripts/helpers.sh"

DOCU_PATH="$1"
shift
DOCU_NAME=$(basename ${0} '.sh')
MAIN_TOUPDATE="${DOCU_PATH}/${DOCU_NAME}-toupdate.dan"
DOWNLOAD_LINKS=(
https://developers.google.com
)
# -------------------------------------
# eof eof eof DECLARING VARIABLES AND PROCESSING ARGS

indexing_rules(){

## 2 STAGE INDEX
## 1st) Spider the Links
## 2nd) Download the links (crawl)

## 1st) Spider the links
## If the index-links list doesnt exists create a newone
for DOWNLOAD_LINK in "${DOWNLOAD_LINKS[@]}"; do
    ntfs_filename=$(echo "${DOWNLOAD_LINK}" | sed 's/[<>:"\/\\|?*]/_/g')
    if [ ! -f "$CURRENT_DIR/../index-links/${ntfs_filename}.csv" ]; then
        echo "No index-links file found, spidering to get a new one" >&2
        echo "This may take a while, and has to be done in one go" >&2
        echo "Consider doing this using a VPS ..." >&2

        wget \
        `## Basic Startup Options` \
          --execute robots=off \
        `## Loggin and Input File Options` \
          --force-html \
        `## Download Options` \
          --spider \
        `## Directory Options` \
        `## HTTP Options` \
          --user-agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36 Edg/91.0.864.59" \
        `## HTTPS Options` \
          --no-check-certificate \
        `## Recursive Retrieval Options` \
          --recursive --level=inf \
          --delete-after \
        `## Recursive Accept/Reject Options` \
          --reject '*.jpg,*.svg,*.js,*.json,*.css,*.png,*.xml,*.txt,*.mp4,*.gif,*.webp,*.ico,*.woff,*.woff2,*.ttf,*.pdf,*.java,*.sql,*.jar,*.zip,*.GIF,*.PNG,*.svgz,*.webp,*.mp4,*.ico,*.gif,*.jpg,*.svg,*.js,*.json,*.css,*.png,*.xml,*.txt,*.webp,*.mp4,*.ico,*.gif,*.jpg,*.svg,*.js,*.json,*.css,*.png,*.xml,*.txt' \
          --reject-regex '.*?hel=.*|.*?hl=.*|.*[%&=?].*|\\\"' \
          "${DOWNLOAD_LINK}" 2>&1  \
              | grep '^--' | awk '{ print $3 }' | awk '{ print $0 ",-1" }' | sort -u \
          > "$CURRENT_DIR/../index-links/${ntfs_filename}.csv"
    fi
done

#### 2nd) Download the links (crawl)


if [ ! -d "${DOCU_PATH}/downloaded" ]; then
    mkdir -p "${DOCU_PATH}/downloaded"
fi

for DOWNLOAD_LINK in "${DOWNLOAD_LINKS[@]}"; do

    ntfs_filename=$(echo "${DOWNLOAD_LINK}" | sed 's/[<>:"\/\\|?*]/_/g')


    ## Check if a local csv has copied, if not copy the one from the repository 
    LOCAL_CSV_PATH="${DOCU_PATH}/${ntfs_filename}.csv"
    if [ ! -f ${DOCU_PATH}/${ntfs_filename}.csv ]; then
        echo "No previous local csv has been found" >&2
        echo "Starting the index from the scratch ..." >&2
        cp "$CURRENT_DIR/../index-links/${ntfs_filename}.csv" "${LOCAL_CSV_PATH}"
    else
        echo "Resuming a previous download ..." >&2
    fi


    ## We are iterating through the .csv line by line
    total_lines=$(wc -l < ${LOCAL_CSV_PATH} ) 

    for ((row_no=1; row_no<=total_lines; row_no++)); do
        row=$(sed -n "${row_no}p" "${LOCAL_CSV_PATH}" )  # Extract the specific row number $row_no

        ## Parsing url and exit_status
        url=$(echo "$row" | cut -d',' -f1)
        exit_status=$(echo "$row" | cut -d',' -f2)

        
        ## If the file hasnt been attempted to download yet
        if [ "$exit_status" -eq -1 ]; then 
            url=$(echo "$row" | cut -d',' -f1)
            dirpath=$(echo "$url" | sed 's|https://||;s|/[^/]*$||')
            filename=$(basename "$url")

            ## If the file doesnt exist locally
            if [ ! -f ${DOCU_PATH}/downloaded/${dirpath}/${url} ]; then
                wget -nc --adjust-extension --directory-prefix=${DOCU_PATH}/downloaded/${dirpath} ${url} > /dev/null 2>&1
                wget_status=${?}
            else ## File was already downloaded,update the wget_status
                echo "Found the file locally, despite not been in the list : ${url}" >&2
                wget_status="0"
            fi

            ## Update in-place the row of the file with the new wget_status
            row="${url},${wget_status}"

            echo "Updated row : ${row}" >&2

            sed -i "${row_no}c\\${row}" "${LOCAL_CSV_PATH}"
        fi
    done
done


}

arranging_rules(){


    ## Making a backup of the index, if it doesnt exists
    if [[ ! -d "${DOCU_PATH}/downloaded-bk" || ! "$(ls -A "${DOCU_PATH}/downloaded-bk" 2>/dev/null)" ]]; then
        cp -r ${DOCU_PATH}/downloaded ${DOCU_PATH}/downloaded-bk/
    fi


    # DOCUMENTATION SPECIFIC RULES
    # ---------------------------------------------------------------------------
    # Add below statments regarding to arranging_rules that are specific for this documentation

    # Example, you want to remove the blog section that has been indexed
    # rm -r ${DOCU_PATH}/downloaded/blog

    

    # EOF EOF EOF DOCUMENTATION SPECIFIC RULES
    # ---------------------------------------------------------------------------



    ## Files cleanup

    ## Clean up the duplicate files
    # Keeping the least nested one
    jdupes -r -N -d ${DOCU_PATH}/downloaded/


    ## Modifying documents
    rename_lone_index
    deestructuring_dir_tree
}


parsing_rules(){

    write_header
    ## Change below the html tags to be parsed -f for titles , -b for body
    # Example: 
    #    We parse the Titles of the Topics by using 'h1'
    #    We parse the Content of the Pages by using 'article'
    
    #    write_html_docu_multirule -f "h1" -b "article" -cp
    #
    #  Other Example:
    #    You may use various tags, the firstone to be found will be used
    #    In the documentation downloaded some pages are different than others
    #        The Content of the Pages sometimes is under "div.guide-content" sometimes under "body"
    #
    #    write_html_docu_multirule -f "head title" -b "div.guide-content" -b "body" -cp
    #
    write_html_docu_multirule -f "" -b "" -cp


    # DOCUMENT CLEANUP RULES
    # ---------------------------------------------------------------------------
    ## Retrieving content of the files and cleaning it
    ## Change below patterns of text to be cleaned from the main document
    ## 
    ## For example the below patterns are used for
    ##     Removing ¶
    ##     Removing <200b>
    ##
    ## Change accordingly

    sed -e 's/[[:space:]]\+¶//g' \
        -e "s/$(echo -ne '\u200b')//g" \
        -i "${MAIN_TOUPDATE}"

    # EOF EOF EOF DOCUMENT CLEANUP RULES
    # ---------------------------------------------------------------------------

    write_ext_modeline


    # MODELINE FOR SYNTAX SPECIFIC PATTERNS FOR KEYWORDS
    # ---------------------------------------------------------------------------
    # Change below the keywords you want to be highlighted
    #    The syntax highlighting will be using Default groups , such as Question , NonText etc...
    #        Introduce the patterns that you want to be highlighted with the same style than those groups
    #
    #    For instance to highlight the Pattern "^Reference" and "^Example:" which are repeated throughout the documentation 
    #    In this example I want to highlight it with Green Letter, normal foreground. 
    #         I will be using then the Question group
    #         So Using coma separated
    #           
    #    sed -i '11a\'$'\n''&@ g:dan_kw_question_list = "^Example:,^Reference"" @&' "${MAIN_TOUPDATE}"
    #    Note : there are Character limitations ",= and others wont be working
    
    sed -i '10a\'$'\n''&@ The following lines are used by vim-dan, do not modify them! @&' "${MAIN_TOUPDATE}"
    sed -i '12a\'$'\n''&@ g:dan_kw_question_list = "" @&' "${MAIN_TOUPDATE}"
    sed -i '13a\'$'\n''&@ g:dan_kw_nontext_list = "" @&' "${MAIN_TOUPDATE}"
    sed -i '14a\'$'\n''&@ g:dan_kw_linenr_list = "" @&' "${MAIN_TOUPDATE}"
    sed -i '15a\'$'\n''&@ g:dan_kw_warningmsg_list = "" @&' "${MAIN_TOUPDATE}"
    sed -i '16a\'$'\n''&@ g:dan_kw_colorcolumn_list = "" @&' "${MAIN_TOUPDATE}"
    sed -i '17a\'$'\n''&@ g:dan_kw_underlined_list = "" @&' "${MAIN_TOUPDATE}"
    sed -i '18a\'$'\n''&@ g:dan_kw_preproc_list = "" @&' "${MAIN_TOUPDATE}"
    sed -i '19a\'$'\n''&@ g:dan_kw_comment_list = "" @&' "${MAIN_TOUPDATE}"
    sed -i '20a\'$'\n''&@ g:dan_kw_identifier_list = "" @&' "${MAIN_TOUPDATE}"
    sed -i '21a\'$'\n''&@ g:dan_kw_ignore_list = "" @&' "${MAIN_TOUPDATE}"
    sed -i '22a\'$'\n''&@ g:dan_kw_statement_list = "" @&' "${MAIN_TOUPDATE}"
    sed -i '23a\'$'\n''&@ g:dan_kw_cursorline_list = "" @&' "${MAIN_TOUPDATE}"
    sed -i '24a\'$'\n''&@ g:dan_kw_tabline_list = "" @&' "${MAIN_TOUPDATE}"

    # EOF EOF EOF MODELINE FOR SYNTAX SPECIFIC PATTERNS FOR KEYWORDS
    # ---------------------------------------------------------------------------
}

## PARSING ARGUMENTS
## ------------------------------------
# (do not touch)
while getopts ":siap" opt; do
    case ${opt} in
        s)
            spidering_rules
            ;;
        i)
            indexing_rules
            ;;
        a)
            arranging_rules
            ;;
        p)
            parsing_rules
            ;;
        h | *)
            echo "Usage: $0 [-s] [-i] [-a] [-p] [-h] "
            echo "Options:"
            echo "  -s  Spidering"
            echo "  -i  Indexing"
            echo "  -a  Arranging"
            echo "  -p  Parsing"
            echo "  -h  Help"
            exit 0
            ;;
    esac
done
## EOF EOF EOF PARSING ARGUMENTS
## ------------------------------------
