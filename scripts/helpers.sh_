#!/bin/bash 
# @file helpers.sh
# @brief Helper functions in use on vim-dan-generator.
# @description
#   author: rafmartom <rafmartom@gmail.com>



## ----------------------------------------------------------------------------
# @section External_subroutines
# @description Sub-routines that are called by external files
#   They are mainly called by the main file
#   Thus they are triggered by the main stages of the Dan Documentation Generation

# @description Peforms a local installation of an already generated documentation
# @option -n <DOCU_NAME> The name of the vim-dan Documentation. Defined by the filename created on /documentations/docu-name.sh
# @option -d <VIMDAN_DIR> Local directory target installation of the vim-dan documents, defined in /.vimdan_config
# @option -v <VIM_RTP_DIR> Target directory for the vim configuration filesm, defined in .vimdan_config
# @stdout Return Description
# @example perform_install -n -p 
function perform_install() {
    ## PARSING FUNCTION ARGUMENTS ------------------------
    local OPTIND=1
    # Initialize variables
    DOCU_NAME=""
    VIMDAN_DIR=""
    VIM_RTP_DIR=""

    while getopts "n:d:v:" opt; do
        case "${opt}" in
            n) DOCU_NAME="${OPTARG}" ;;
            d) VIMDAN_DIR="${OPTARG}" ;;
            v) VIM_RTP_DIR="${OPTARG}" ;;
            *)
                echo "Usage: perform_install -n DOCU_NAME -d VIMDAN_DIR -v VIM_RTP_DIR" >&2
                echo "Example: perform_install -n DOCU_NAME -d VIMDAN_DIR -v VIM_RTP_DIR" >&2
                return 1
                ;;
        esac
    done
    # Variable check
    if [[ -z "${DOCU_NAME}" || -z "${VIMDAN_DIR}" || -z "${VIM_RTP_DIR}" ]]; then
        echo "Error: perform_install requires -n DOCU_NAME -d VIMDAN_DIR -v VIM_RTP_DIR" >&2
        return 1
    fi
    ## EOF EOF EOF PARSING FUNCTION ARGUMENTS -------------
    process_paths

    [ ! -d "${VIMDAN_DIR}" ] && mkdir -p "${VIMDAN_DIR}"
    echo "Installing vim-dan ${DOCU_NAME} into ${VIMDAN_DIR}/ ..."

    # If the file is compressed extract
    if [ -e "$CURRENT_DIR"/ready-docus/"${DOCU_NAME}.dan.bz2" ]; then
        echo "There is compression file"
        bunzip2 -kc "$CURRENT_DIR"/ready-docus/"${DOCU_NAME}.dan.bz2" > ${VIMDAN_DIR}/${DOCU_NAME}-toupdate.dan
    else 
        cp $CURRENT_DIR/ready-docus/${DOCU_NAME}.dan ${VIMDAN_DIR}/${DOCU_NAME}-toupdate.dan
    fi
    perform_patch
    update_tags
    updating_vim
    install_autoload
}

# @description Updates the local file of a certain documentation, keeping when possible the dan-highlighted lines
# @option -n <DOCU_NAME> The name of the vim-dan Documentation. Defined by the filename created on /documentations/docu-name.sh
# @option -d <VIMDAN_DIR> Local directory target installation of the vim-dan documents, defined in /.vimdan_config
# @option -v <VIM_RTP_DIR> Target directory for the vim configuration filesm, defined in /.vimdan_config
# @stdout Return Description
# @example perform_update -n -p 
function perform_update() {
    ## PARSING FUNCTION ARGUMENTS ------------------------
    local OPTIND=1
    # Initialize variables
    DOCU_NAME=""
    VIMDAN_DIR=""
    VIM_RTP_DIR=""

    while getopts "n:d:v:" opt; do
        case "${opt}" in
            n) DOCU_NAME="${OPTARG}" ;;
            d) VIMDAN_DIR="${OPTARG}" ;;
            v) VIM_RTP_DIR="${OPTARG}" ;;
            *)
                echo "Usage: perform_update -n DOCU_NAME -d VIMDAN_DIR -v VIM_RTP_DIR" >&2
                echo "Example: perform_update -n DOCU_NAME -d VIMDAN_DIR -v VIM_RTP_DIR" >&2
                return 1
                ;;
        esac
    done
    # Variable check
    if [[ -z "${DOCU_NAME}" || -z "${VIMDAN_DIR}" || -z "${VIM_RTP_DIR}" ]]; then
        echo "Error: perform_update requires -n DOCU_NAME -d VIMDAN_DIR -v VIM_RTP_DIR" >&2
        return 1
    fi
    ## EOF EOF EOF PARSING FUNCTION ARGUMENTS -------------
    process_paths


    echo "Updating vim-dan ${DOCU_NAME} on ${VIMDAN_DIR}/ ..."
    echo "Note!! . In order for this to have any effect you previously should have updated the repository"
    cp $CURRENT_DIR/ready-docus/${DOCU_NAME}.dan ${VIMDAN_DIR}/${DOCU_NAME}-toupdate.dan
    perform_patch
    update_tags
    updating_vim
    install_autoload


}

# @description Performs the spidering stage of a given documentation:
#   - For "vim-dan" spidering means the process of gathering all the links of the Files that are going to make the 
#   documentation topics.
#       It creates .csv file with all the links to be downloaded in /index-links/<site>.csv.bz2
#
#  Note that Child documentations dont have spidering_rules:
#      - Parent documentations (the ones named -parent.sh )only have : spidering_rules and indexing_rules  
#      - Child documentations only have : arranging_rules and parsing_rules 
#
#   - The spidering rules are specific to each documentation and are defined in
#        /documentations/docu-name.sh:spidering_rules()
#   The fact that the ruleset is defined on a per documentation basis gives you the ability to create your own.
#   Each documentation may have its own quirks, but the bast majority of sites will work nicely with the
#   already established ruleset available /documentations/template.sh:
#   In case of creating a new documentation from the scratch start from this file, changing just the DOWNLOAD_LINK.
#   If there are issues in any of the stages, then you may need to adapt these rules.
# @option -n <DOCU_NAME> The name of the vim-dan Documentation. Defined by the filename created on /documentations/docu-name.sh
# @option -d <VIMDAN_DIR> Local directory target installation of the vim-dan documents, defined in /.vimdan_config
# @stdout Return Description
# @example perform_spider -n -p 
function perform_spider() {
    ## PARSING FUNCTION ARGUMENTS ------------------------
    local OPTIND=1
    # Initialize variables
    DOCU_NAME=""
    VIMDAN_DIR=""

    while getopts "n:d:v:" opt; do
        case "${opt}" in
            a) DOCU_NAME="${OPTARG}" ;;
            d) VIMDAN_DIR="${OPTARG}" ;;
            *)
                echo "Usage: perform_spider -n DOCU_NAME -d VIMDAN_DIR" >&2
                echo "Example: perform_spider -n DOCU_NAME -d VIMDAN_DIR" >&2
                return 1
                ;;
        esac
    done
    # Variable check
    if [[ -z "${DOCU_NAME}" || -z "${VIMDAN_DIR}" ]]; then
        echo "Error: perform_spider requires -n DOCU_NAME -d VIMDAN_DIR" >&2
        return 1
    fi
    ## EOF EOF EOF PARSING FUNCTION ARGUMENTS -------------
    process_paths

    echo "Spidering vim-dan ${DOCU_NAME} on $CURRENT_DIR/../index-links/"
    ${CURRENT_DIR}/documentations/${DOCU_NAME}.sh ${DOCU_PATH} "-s"

    # Logging that into the documentation-status file
    update_csv_field -f "$CURRENT_DIR/documentation-status.csv" -r "${DOCU_NAME}" -c "last_spidered" -i $(date +"%Y%m%d_%H%M%S") -n

}

# @description Performs the indexing stage of a given documentation
#   - For "vim-dan" indexing is the process of downloading each and every file of the documentation.
#   By following a link-list like the ones created on the spidering process the download can be
#     paused, halted and resumed other day.
#   The files will be downloaded on ${VIMDAN_DIR}/${DOCU_NAME}/downloaded
#   The indexing process can also take place directly, without a prior spider.
#
#  Note that Child documentations dont have indexing_rules:
#      - Parent documentations (the ones named -parent.sh )only have : spidering_rules and indexing_rules  
#      - Child documentations only have : arranging_rules and parsing_rules 
#
#   - The indexing rules are specific to each documentation and are defined in
#        /documentations/docu-name.sh:indexing_rules()
#   The fact that the ruleset is defined on a per documentation basis gives you the ability to create your own.
#   Each documentation may have its own quirks, but the bast majority of sites will work nicely with the
#   already established ruleset available /documentations/template.sh:
#   In case of creating a new documentation from the scratch start from this file, changing just the DOWNLOAD_LINK.
#   If there are issues in any of the stages, then you may need to adapt these rules.
# @option -n <DOCU_NAME> The name of the vim-dan Documentation. Defined by the filename created on /documentations/docu-name.sh
# @option -d <VIMDAN_DIR> Local directory target installation of the vim-dan documents, defined in /.vimdan_config
# @stdout Return Description
# @example perform_index -n -p 
function perform_index() {
    ## PARSING FUNCTION ARGUMENTS ------------------------
    local OPTIND=1
    # Initialize variables
    DOCU_NAME=""
    VIMDAN_DIR=""

    while getopts "n:d:v:" opt; do
        case "${opt}" in
            a) DOCU_NAME="${OPTARG}" ;;
            d) VIMDAN_DIR="${OPTARG}" ;;
            *)
                echo "Usage: perform_index -n DOCU_NAME -d VIMDAN_DIR" >&2
                echo "Example: perform_index -n DOCU_NAME -d VIMDAN_DIR" >&2
                return 1
                ;;
        esac
    done
    # Variable check
    if [[ -z "${DOCU_NAME}" || -z "${VIMDAN_DIR}" ]]; then
        echo "Error: perform_index requires -n DOCU_NAME -d VIMDAN_DIR" >&2
        return 1
    fi
    ## EOF EOF EOF PARSING FUNCTION ARGUMENTS -------------
    process_paths

    echo "Indexing vim-dan ${DOCU_NAME} on ${DOCU_PATH}/ ..."
    ${CURRENT_DIR}/documentations/${DOCU_NAME}.sh ${DOCU_PATH} "-i"

    # Logging that into the documentation-status file
    update_csv_field -f "$CURRENT_DIR/documentation-status.csv" -r "${DOCU_NAME}" -c "last_indexed" -i $(date +"%Y%m%d_%H%M%S") -n


}

# @description Performs the arranging process of a given documentation
#   - For "vim-dan" arranging is the process of moving, and deleting of the files that have been download in the Index.
#   On the early versions of "vim-dan" this stage was more important, now thanks to the evolution of the algorithm there 
#   is no need to perform as many actions.
#   Actualy some actions are not desirable, such as moving files from one subdir to other will probably break the link parsing functionalities. 
#  This process creates a backup of the index that in case the rule-set is wrong, you can use without having to re-index it.
#   ${VIMDAN_DIR}/${DOCU_NAME}/downloaded-bk
#   After that, it deletes files that are not relevant for the documentation that have been downloaded.
#   In many cases if the documentation is only made of one DOWNLOAD_LINK (as many are) it is useful to bring down all the subdirectories from original the host directory deleting this.
#   At the end it cleans the duplicates.
#
#  Note that Parent documentations dont have arranging_rules:
#      - Parent documentations (the ones named -parent.sh )only have : spidering_rules and indexing_rules  
#      - Child documentations only have : arranging_rules and parsing_rules 
#
#   - The arranging rules are specific to each documentation and are defined in
#        /documentations/docu-name.sh:arranging_rules()
#   The fact that the ruleset is defined on a per documentation basis gives you the ability to create your own.
#   Each documentation may have its own quirks, but the bast majority of sites will work nicely with the
#   already established ruleset available /documentations/template.sh:
#   In case of creating a new documentation from the scratch start from this file, changing just the DOWNLOAD_LINK.
#   If there are issues in any of the stages, then you may need to adapt these rules.
# @option -n <DOCU_NAME> The name of the vim-dan Documentation. Defined by the filename created on /documentations/docu-name.sh
# @option -d <VIMDAN_DIR> Local directory target installation of the vim-dan documents, defined in /.vimdan_config
# @stdout Return Description
# @example perform_arrange -n -p 
function perform_arrange() {
    ## PARSING FUNCTION ARGUMENTS ------------------------
    local OPTIND=1
    # Initialize variables
    DOCU_NAME=""
    VIMDAN_DIR=""

    while getopts "n:d:v:" opt; do
        case "${opt}" in
            a) DOCU_NAME="${OPTARG}" ;;
            a) VIMDAN_DIR="${OPTARG}" ;;
            *)
                echo "Usage: perform_arrange -n DOCU_NAME -d VIMDAN_DIR" >&2
                echo "Example: perform_arrange -n DOCU_NAME -d VIMDAN_DIR" >&2
                return 1
                ;;
        esac
    done
    # Variable check
    if [[ -z "${DOCU_NAME}" || -z "${VIMDAN_DIR}" ]]; then
        echo "Error: perform_arrange requires -n DOCU_NAME -d VIMDAN_DIR" >&2
        return 1
    fi
    ## EOF EOF EOF PARSING FUNCTION ARGUMENTS -------------
    process_paths


    echo "Arranging vim-dan ${DOCU_NAME} files on ${DOCU_PATH}/ ..."
    ${CURRENT_DIR}/documentations/${DOCU_NAME}.sh ${DOCU_PATH} "-a"

    # Logging that into the documentation-status file
    update_csv_field -f "$CURRENT_DIR/documentation-status.csv" -r "${DOCU_NAME}" -c "last_arranged" -i $(date +"%Y%m%d_%H%M%S") -n

}



# @description Performs the parsing and writting stage of a given documentation
#   - For "vim-dan" parsing means, obtaining the necessary text from each downloaded file, such as File "Titles", and Content
#    , taking off the rest of the redudant information that each file may have such as navigation bars footers etc...
#    In the most common of the cases are .html files with different Selectors, some matches the Titles, some matches the content.
#    Other stage of parsing involves processing the links.
#    On the writting stage there will be the creation of an interactive TOC
#    To then the appending of each File Relevant Content. All in the `.dan` format.
#    At the end of the writting process there is a cleanup process, for yet some string regularities that couldn't be filtered on the previous process (it typically involve sed commands)
#   To then create the different modelines needed for the functioning of the `.dan` format.
#
#   Once the file has been generated, it will install all the vim-dan files locally if they are not installed.
#
#  Note that parent documentations dont have parsing_rules:
#      - Parent documentations (the ones named -parent.sh )only have : spidering_rules and indexing_rules  
#      - Child documentations only have : arranging_rules and parsing_rules 
#
#   - The parsing rules are specific to each documentation and are defined in
#        /documentations/docu-name.sh:parsing_rules()
#   The fact that the ruleset is defined on a per documentation basis gives you the ability to create your own.
#   Each documentation may have its own quirks, but the bast majority of sites will work nicely with the
#   already established ruleset available /documentations/template.sh:
#   In case of creating a new documentation from the scratch start from this file, changing just the DOWNLOAD_LINK.
#   If there are issues in any of the stages, then you may need to adapt these rules.

# @option -n <DOCU_NAME> The name of the vim-dan Documentation. Defined by the filename created on /documentations/docu-name.sh
# @option -d <VIMDAN_DIR> Local directory target installation of the vim-dan documents, defined in /.vimdan_config
# @option -v <VIM_RTP_DIR> Target directory for the vim configuration filesm, defined in /.vimdan_config
# @stdout Return Description
# @example perform_parse -n -p 
function perform_parse() {
    ## PARSING FUNCTION ARGUMENTS ------------------------
    local OPTIND=1
    # Initialize variables
    DOCU_NAME=""
    VIMDAN_DIR=""
    VIM_RTP_DIR=""

    while getopts "n:d:v:" opt; do
        case "${opt}" in
            n) DOCU_NAME="${OPTARG}" ;;
            d) VIMDAN_DIR="${OPTARG}" ;;
            v) VIM_RTP_DIR="${OPTARG}" ;;
            *)
                echo "Usage: perform_parse -n DOCU_NAME -d VIMDAN_DIR -v VIM_RTP_DIR" >&2
                echo "Example: perform_parse -n DOCU_NAME -d VIMDAN_DIR -v VIM_RTP_DIR" >&2
                return 1
                ;;
        esac
    done
    # Variable check
    if [[ -z "${DOCU_NAME}" || -z "${VIMDAN_DIR}" || -z "${VIM_RTP_DIR}" ]]; then
        echo "Error: perform_parse requires -n DOCU_NAME -d VIMDAN_DIR -v VIM_RTP_DIR" >&2
        return 1
    fi
    ## EOF EOF EOF PARSING FUNCTION ARGUMENTS -------------
    process_paths


    echo "Parsing vim-dan ${DOCU_NAME} on ${VIMDAN_DIR}/ ..."
    ${CURRENT_DIR}/documentations/${DOCU_NAME}.sh ${DOCU_PATH} "-p"
    perform_patch
    update_tags
    updating_vim
    install_autoload

    # Logging that into the documentation-status file
    update_csv_field -f "$CURRENT_DIR/documentation-status.csv" -r "${DOCU_NAME}" -c "last_parsed" -i $(date +"%Y%m%d_%H%M%S") -n
}



# @description It will remove a certain dan documentation localy. It will also remove the dan files.
#   For this last reason is not recommended to use it, as the dan files are one per documentation just delete it manualy.
# @option -n <DOCU_NAME> The name of the vim-dan Documentation. Defined by the filename created on /documentations/docu-name.sh
# @option -d <VIMDAN_DIR> Local directory target installation of the vim-dan documents, defined in /.vimdan_config
# @option -v <VIM_RTP_DIR> Target directory for the vim configuration filesm, defined in /.vimdan_config
# @stdout Return Description
# @example perform_remove -n -p -v 
function perform_remove() {
    ## PARSING FUNCTION ARGUMENTS ------------------------
    local OPTIND=1
    # Initialize variables
    DOCU_NAME=""
    VIMDAN_DIR=""
    VIM_RTP_DIR=""

    while getopts "n:p:v:" opt; do
        case "${opt}" in
            n) DOCU_NAME="${OPTARG}" ;;
            p) VIMDAN_DIR="${OPTARG}" ;;
            v) VIM_RTP_DIR="${OPTARG}" ;;
            *)
                echo "Usage: perform_remove -n DOCU_NAME -p VIMDAN_DIR -v VIM_RTP_DIR" >&2
                echo "Example: perform_remove -n DOCU_NAME -p VIMDAN_DIR -v VIM_RTP_DIR" >&2
                return 1
                ;;
        esac
    done
    # Variable check
    if [[ -z "${DOCU_NAME}" || -z "${VIMDAN_DIR}" || -z "${VIM_RTP_DIR}" ]]; then
        echo "Error: perform_remove requires -n DOCU_NAME -p VIMDAN_DIR -v VIM_RTP_DIR" >&2
        return 1
    fi
    ## EOF EOF EOF PARSING FUNCTION ARGUMENTS -------------
    process_paths

    echo "Removing vim-dan ${DOCU_NAME} off ${DOCU_PATH}/ ..."
    rm -r ${DOCU_PATH}
    rm  ${VIMDAN_DIR}/${DOCU_NAME}.dan
    rm  ${VIMDAN_DIR}/.tags${DOCU_NAME}
    rm ${VIM_RTP_DIR}/ftdetect/dan.vim 
    rm ${VIM_RTP_DIR}/after/ftplugin/dan.vim 
    rm ${VIM_RTP_DIR}/syntax/dan.vim 
    rm ${VIM_RTP_DIR}/autoload/dan.vim 

}

# @description Delete the Indexed files for a certain documentation.
#   Good to save up space once the documentation has been parsed, the index files are not longer needed.
#   Mind that if you are creating a new documentation and are defining a rule-set, if you are unsure that 
#   Some rules may be changed it is better to keep it.
# @option -d <VIMDAN_DIR> Local directory target installation of the vim-dan documents, defined in /.vimdan_config
# @stdout Return Description
# @example delete_index -p 
function delete_index() {
    ## PARSING FUNCTION ARGUMENTS ------------------------
    local OPTIND=1
    # Initialize variables
    VIMDAN_DIR=""

    while getopts "p:" opt; do
        case "${opt}" in
            p) VIMDAN_DIR="${OPTARG}" ;;
            *)
                echo "Usage: delete_index -p VIMDAN_DIR" >&2
                echo "Example: delete_index -p VIMDAN_DIR" >&2
                return 1
                ;;
        esac
    done
    # Variable check
    if [[ -z "${VIMDAN_DIR}" ]]; then
        echo "Error: delete_index requires -p VIMDAN_DIR" >&2
        return 1
    fi
    ## EOF EOF EOF PARSING FUNCTION ARGUMENTS -------------
    process_paths

    echo "Deleting previous Index ..."
    rm -r ${DOCU_PATH}/downloaded

}
## EOF EOF EOF External_subroutines
## ----------------------------------------------------------------------------







## ----------------------------------------------------------------------------
# @section Internal_subroutines
# @description Subroutines that are triggered automatically by other function
#   Note for minimal boilerplate these functions:
#    - Are going to use shared variables
#    - Are not going to have type checking
#    Thus they wont be moveable from this file without re-writting 

# @description Generate shared-variables for use across the script
#   Mostly related to filepaths that are processed through the minimal
#   Arguments needed
function process_paths() {
    MAIN_TOUPDATE="${VIMDAN_DIR}/${DOCU_NAME}-toupdate.dan"
    MAIN_FILE="${VIMDAN_DIR}/${DOCU_NAME}.dan"
    DOCU_PATH="${VIMDAN_DIR}/${DOCU_NAME}"
}

# @description Installs the vim autoload dan file
function install_autoload() {
    if [ ! -f ${VIM_RTP_DIR}/autoload/dan.vim ]; then
        echo "Installing autoload ..."
        [ ! -d ${VIM_RTP_DIR}/autoload ] && mkdir -p ${VIM_RTP_DIR}/autoload 
        cp ${CURRENT_DIR}/autoload/dan.vim ${VIM_RTP_DIR}/autoload/
    fi
}

# @description It attempts to update an outdated dan-file with (X) annotations with a new-one.
#   It maw casuse conflicts, it is recommendable not to mix new versions of dan files with old ones
function perform_patch() {
    echo "Patching the documentation..."
    # Patching the localdocu
    # If performed for the first-time just use the new index
    if [ -e ${MAIN_FILE} ]; then
        cp ${MAIN_FILE} ${MAIN_FILE}-bk
        diff <(sed 's/ (X)$//g' ${MAIN_FILE}) ${MAIN_TOUPDATE} | patch ${MAIN_FILE}
        rm ${MAIN_TOUPDATE}
    else
        mv ${MAIN_TOUPDATE} ${MAIN_FILE}
    fi
}

# @description Update the local tags file for the current documentation
function update_tags() {
    echo "Updating the tag file..."
    ctags --options=NONE --options=${CURRENT_DIR}/ctags-rules/dan.ctags --tag-relative=always -f ${VIMDAN_DIR}/.tags${DOCU_NAME} ${MAIN_FILE} 
    [ ! -d "${HOME}/.ctags.d" ] && mkdir -p "${HOME}/.ctags.d"
    cp ${CURRENT_DIR}/ctags-rules/dan.ctags ${HOME}/.ctags.d/

}

# @description It install/replaces the vim configuration files needed for dan functionalities.
function update_vim() {
    echo "Updating vim files..."
    [ ! -d "${VIM_RTP_DIR}/ftdetect" ] && mkdir -p "${VIM_RTP_DIR}/ftdetect"
    cp ${CURRENT_DIR}/ft-detection/dan.vim  ${VIM_RTP_DIR}/ftdetect/
    [ ! -d "${VIM_RTP_DIR}/after/ftplugin" ] && mkdir -p "${VIM_RTP_DIR}/after/ftplugin"
    cp ${CURRENT_DIR}/linking-rules/dan.vim ${VIM_RTP_DIR}/after/ftplugin/dan.vim
    [ ! -d "${VIM_RTP_DIR}/syntax" ] && mkdir -p "${VIM_RTP_DIR}/syntax"
    cp ${CURRENT_DIR}/syntax-rules/dan.vim  ${VIM_RTP_DIR}/syntax/
}


## EOF EOF EOF Internal_subroutines 
## ----------------------------------------------------------------------------


## ----------------------------------------------------------------------------
# @section Cross-Project Utilities
# @description Utility functions that are usable in other projects

# @description Update a CSV field given a ROW_NAME for the first column and a COL_NAME for the header column.
# If both ROW_NAME and COL_NAME match, the field will be updated with INPUT_STRING.
# If the -n flag is used and ROW_NAME is not found, a new row will be added at the end of the CSV file.
#
# Example CSV:
#   ,notes,last_parsed,last_arranged
#   adobe-ae,,,,
#   patxie,,,,
#
# Example usage:
#   update_csv_field -f "example.csv" -r "adobe-ae" -c "last_parsed" -i "now"
#
# Result:
#   ,notes,last_parsed,last_arranged
#   adobe-ae,,now,,
#   patxie,,,,
#
# @option -f <CSV_FILENAME> The path to the CSV file to update.
# @option -r <ROW_NAME> The name of the row to update (matches the first column).
# @option -c <COL_NAME> The name of the column to update (matches the header row).
# @option -i <INPUT_STRING> The new value to insert into the specified field.
# @option -n Add a new row if ROW_NAME is not found in the CSV file.
# @stdout No direct output. The CSV file is modified in place.
# @example update_csv_field -f "example.csv" -r "adobe-ae" -c "last_parsed" -i "now"
# @example update_csv_field -f "example.csv" -r "new-row" -c "notes" -i "added" -n
function update_csv_field() {
    ## PARSING FUNCTION ARGUMENTS ------------------------
    ## ---------------------------------------------------
    local OPTIND=1
    # Initialize variables
    CSV_FILENAME=""
    ROW_NAME=""
    COL_NAME=""
    INPUT_STRING=""
    NEW_ROW=""

    while getopts "f:r:c:i:n" opt; do
        case "${opt}" in
            f) CSV_FILENAME="${OPTARG}" ;;
            r) ROW_NAME="${OPTARG}" ;;
            c) COL_NAME="${OPTARG}" ;;
            i) INPUT_STRING="${OPTARG}" ;;
            n) NEW_ROW=1 ;;  # Flag to indicate a new row should be added if ROW_NAME is not found
            *)
                echo "Usage: update_csv_field -f CSV_FILENAME -r ROW_NAME -c COL_NAME -i INPUT_STRING [-n]" >&2
                echo "Example: update_csv_field -f CSV_FILENAME -r ROW_NAME -c COL_NAME -i INPUT_STRING [-n]" >&2
                return 1
                ;;
        esac
    done
    # Variable check
    if [[ -z "${CSV_FILENAME}" || -z "${ROW_NAME}" || -z "${COL_NAME}" || -z "${INPUT_STRING}" ]]; then
        echo "Error: update_csv_field requires -f CSV_FILENAME -r ROW_NAME -c COL_NAME -i INPUT_STRING" >&2
        return 1
    fi
    ## PARSING FUNCTION ARGUMENTS ------------------------
    ## EOF EOF EOF EOF------------------------------------

    TMP_FILE=$(mktemp)
    dos2unix "${CSV_FILENAME}" 2> /dev/null

    awk -F',' -v OFS=',' -v row_name="$ROW_NAME" -v col_name="$COL_NAME" -v input_string="$INPUT_STRING" -v new_row="$NEW_ROW" '
    NR==1 {
        for (i=1; i<=NF; i++) if ($i == col_name) col=i;
        total_cols=NF;  # Store total number of columns
        print;
        next;
    }
    $1 == row_name && col {
        $col = input_string;
        found_row=1;
    }
    { print }
    END {
        if (new_row == 1 && !found_row && col) {
            printf "%s", row_name;
            for (i=2; i<=total_cols; i++) {
                if (i == col) {
                    printf ",%s", input_string;
                } else {
                    printf ",";
                }
            }
            printf "\n";
        }
    }' "$CSV_FILENAME" > "$TMP_FILE" && mv "$TMP_FILE" "$CSV_FILENAME"

    [ -f "$TMP_FILE" ] && rm -f "$TMP_FILE"
}

# @description Converts a decimal number to a base-62 alphanumeric string.
# This function maps decimal numbers to a custom base-62 encoding using the characters 0-9, a-z, and A-Z.
# It is useful for generating short, unique identifiers from numeric values.
#
# @arg $1 <num> The decimal number to convert.
# @stdout The base-62 alphanumeric representation of the input number.
# @example decimal_to_alphanumeric 12345  # Converts 12345 to a base-62 string
function decimal_to_alphanumeric() {
    local num=$1
    ALPHANUMERIC=($(echo {0..9} {a..z} {A..Z}))
    local base=${#ALPHANUMERIC[@]}
    local result=""

    while [ $num -gt 0 ]; do
        remainder=$((num % base))
        result="${ALPHANUMERIC[remainder]}$result"
        num=$((num / base))
    done

    echo "$result"
}





## EOF EOF EOF Cross-Project Utilities
## ----------------------------------------------------------------------------


## ----------------------------------------------------------------------------
# @section Deprecated Functions
# @description The following functions are no longer used in this project
#   Although it is worth keeping, as they may be useful again.
#   Or they are well written algorithms worth to be re-checked


# @description Transforms a relative HTML link such as /guidance/living-in-europe
# into & @guidance-)living-in-europe.html@ living-in-europe &
rellink_to_danlinkfrom () {
    rel_link=$1
    linkto_prefix=$(echo "${rel_link}" | sed 's/^\///' | sed -E 's/(.*)(\.html)?$/\1.html/')
    linkto_title=$(basename "${rel_link}")
    danlinkfrom="& @${linkto_prefix}@ ${linkto_title} &"
    echo "${danlinkfrom}"
}


# @description Parses an HTML file to generate an in-file HTML list of "danlinkfrom"-formatted links.
# This function is used during the arrangement process and cannot use `cut-dirs`. It appends a list of
# filtered links to the specified HTML file, wrapped in a `.dan-seealso` div.
#
# The function extracts `href` attributes from elements matching the given CSS selector and filters
# them based on include paths. If no include paths are provided, all relative links are included.
#
# Example usage:
#   parse_dan_seealso "./downloaded/spain.html" ".govuk-link attr{href}" "government/collections" "guidance"
#
# This will parse the `href` attributes from elements with the class `.govuk-link` and include only
# those links that start with `/government/collections` or `/guidance`.
#
# @arg $1 <file> The path to the HTML file to process.
# @arg $2 <selector> The CSS selector to extract links (e.g., ".govuk-link attr{href}").
# @arg $@ <includes> Optional list of include paths to filter links (e.g., "government/collections").
# @stdout Appends an HTML list of filtered links to the input file.
# @example parse_dan_seealso "./downloaded/spain.html" ".govuk-link attr{href}" "government/collections" "guidance"
# @example parse_dan_seealso "./downloaded/spain.html" ".govuk-link attr{href}"  # Includes all relative links
parse_dan_seealso() {
    file=$1
    selector=$2
    shift 2  # Shift to move past the first two arguments

    # Convert the remaining arguments into an array
    includes=("$@")

    # Parse links
    temp_links=$(cat "${file}" | pup "${selector}")

    # Check if the temporary variable is not empty
    if [[ -n "$temp_links" ]]; then
        # De-structure the multiline string into an array
        links_array=()
        while IFS= read -r link; do
            links_array+=("$link")
        done <<< "$temp_links"
    fi

    # Filter the links according to the array of includes
    filtered_links=()

    for link in "${links_array[@]}"; do
        # Skip absolute URLs (external links)
        if [[ "$link" == http://* || "$link" == https://* ]]; then
            continue
        fi

        # If no includes are given, add all relative links
        if [[ ${#includes[@]} -eq 0 ]]; then
            filtered_links+=("$link")
            continue
        fi

        # Otherwise, check if the link starts with any of the allowed paths
        for pattern in "${includes[@]}"; do
            if [[ "$link" == /"$pattern"* ]]; then
                filtered_links+=("$link")
                break  # Stop checking other patterns once a match is found
            fi
        done
    done

    # Append the HTML content to the file
    cat <<EOF >> "$file"
<div class="dan-seealso">
    <p>DAN See Also Links:</p>
    <ul>
EOF

    # Loop through filtered links and append each link as a list item
    for filtered_link in "${filtered_links[@]}"; do
        cat <<EOF >> "$file"
        <li>$(rellink_to_danlinkfrom "${filtered_link}")</li>
EOF
    done

    # Close the HTML tags
    cat <<EOF >> "$file"
    </ul>
</div>
EOF
}


# @description Renames lone `index.html` files to match their parent directory names and moves them one level up.
# This function is useful for restructuring downloaded HTML files where `index.html` files are nested in subdirectories.
# For example:
#   - Input:  www.zaproxy.org/docs/alerts/0/index.html
#   - Output: www.zaproxy.org/docs/alerts/0.html
#
# If the function creates a file named `${DOCU_PATH}/downloaded.html`, it moves it back to `${DOCU_PATH}/downloaded/index.html`.
#
# @arg $DOCU_PATH The base directory containing the downloaded files.
# @stdout No direct output. Files are renamed and moved in place.
# @example rename_lone_index
rename_lone_index() {
    mapfile -t files_array < <(find "${DOCU_PATH}/downloaded/" -type f -name "index.html")
    for file in "${files_array[@]}"; do
        parent="$(basename "$(dirname ${file})")"
        dirname=$(dirname ${file})
        ext=${file##*.}
        mv ${file} "$dirname/../$parent.$ext";
    done

    # If last has created ${DOCU_PATH}/downloaded.html , put it back to downloaded/
    [ -f ${DOCU_PATH}/downloaded.html ] && mv ${DOCU_PATH}/downloaded.html ${DOCU_PATH}/downloaded/index.html
}

# @description Flattens a directory tree by moving files from nested subdirectories to their parent directories.
# This function renames files to include their parent directory names, effectively "de-structuring" the directory hierarchy.
#
# For example:
#   - Input:  www.zaproxy.org/docs/desktop/addons/access-control-testing/contextoptions.html
#   - Output: www.zaproxy.org/docs/desktop/addons/access-control-testing-contextoptions.html
#
# The function iterates up to 15 times to ensure all nested files are moved. After flattening, it deletes any empty directories.
#
# @arg $DOCU_PATH The base directory containing the downloaded files.
# @stdout No direct output. Files are moved and renamed in place, and empty directories are deleted.
# @example deestructuring_dir_tree
deestructuring_dir_tree() {
    # De-estructure all the directory hierarchy
    for i in {1..15}; do
        mapfile -t files_array < <(find "${DOCU_PATH}/downloaded" -mindepth 2 -type f)

        for file in "${files_array[@]}"; do
            parent="$(basename "$(dirname ${file})")"
            dirname=$(dirname ${file})
            mv ${file} "$dirname/../"${parent}"-)$(basename ${file})";
        done
    done

    # Pruning off the empty directories
    find "${DOCU_PATH}/downloaded/" -type d -empty -delete
}

function write_docu_multirule() {
  ## MULTI-FILE PARSING WITH MULTI-RULE
  ## Parsing into an associative array each title and path
  ## With this we can:
  ##      - Create an ordered automated index linkFrom
  ##      - Append each topic content with a linkTo and a figlet header
  ##
  ## Also, there are different parsing rules (multi-rule)
  ##      - Meaning they will be applied to each file sequentially
  ##      - Upon one parsing rule returning non-zero, that parsed title will be added
  ## Example of usage
  ##
  ## write_docu_multirule -f "pup -i 0 --pre 'head title' | pandoc -f html -t plain" \
  ##                      -b "pup -i 0 --pre 'div.guide-content' | pandoc -f html -t plain" \
  ##                      -b "pup -i 0 --pre 'body' | pandoc -f html -t plain"


  local title_parsing_array=()
  local content_parsing_array=()

  ## Parse options for title and body rules
  while [[ "$#" -gt 0 ]]; do
    case "$1" in
      -f)
        if [[ -n "$2" ]]; then
          title_parsing_array+=("$2")
          shift 2
        else
          echo "Error: Missing argument for -f" >&2
          return 1
        fi
        ;;
      -b)
        if [[ -n "$2" ]]; then
          content_parsing_array+=("$2")
          shift 2
        else
          echo "Error: Missing argument for -b" >&2
          return 1
        fi
        ;;
      *)
        echo "Unknown option: $1" >&2
        return 1
        ;;
    esac
  done

  ## Finding all HTML files within a specific directory, sorted in version order
  mapfile -t files_array < <(find "${DOCU_PATH}/downloaded" -type f -name "*.html" | sort -V)

  ## First create the title array
  title_array=()
  for file in "${files_array[@]}"; do

    # (Multi-rule) Parsing functions, add as many as needed
    found_selector=""
    for title_parsing in "${title_parsing_array[@]}"; do
      # Making sure that the titles don't have line breaks
      title=$(eval "$title_parsing" < "$file" | sed ':a;N;$!ba;s/\n/ /g')
      if [ -n "$title" ]; then
        found_selector=true
        break
      fi
    done

    # Default case for parsing, if none of the rules return a non-zero string
    if [ -z "$found_selector" ]; then
      title=$(basename "$file" | cut -f 1 -d '.')
    fi

    # Append the title to the title_array
    title_array+=("$title")
  done

  ## Creating an associative array to map titles to file paths
  declare -A paths_linkto

  # Iterate through the indices of 'files_array'
  for index in "${!files_array[@]}"; do
    file="${files_array[$index]}"
    title="${title_array[$index]}"
    paths_linkto["$file"]="$title"
  done

  # Create the linkFrom index header using figlet
  echo "index" | figlet >> "$MAIN_TOUPDATE"
  prev_dirs_array=()

  ## Splitting directories from filenames and building nested lists
  for file in "${files_array[@]}"; do

    filename=$(basename "$file")
    unset dirs_array
    remaining_file="$filename"

    for ((i=0; i<25; i++)); do
      if [[ "$remaining_file" =~ ([^\)]+)-\)(.+) ]]; then
        dirs_array[i]="${BASH_REMATCH[1]}"
        remaining_file="${BASH_REMATCH[2]}"
      else
        dirs_array[i]="$remaining_file"
        break
      fi
    done

    ## In order to create from this
    ##"workspace-)cse-)reference-)wrap"
    ##"workspace-)cse-)reference-)wrap-private-key"
    ##"workspace-)events-)docs-)release-notes"
    ##
    ## to this
    ##- workspace
    ##    - cse
    ##        - reference
    ##            - wrap
    ##            - wrap-private-key
    ##    - events
    ##        - docs
    ##            -release-notes
    ##
    ## We need to check dirs_array member by member and see what is the indentation
    ## index in which the directory structure differs
    ##
    ##
    ## defining dirs_arrays
    ## defining prev_dirs_array
    ##      Comparing them we calculate
    ##      first_discrepancy_level
    ##          the nesting level at which dirst_array and prev_dirs_array differ
    ##
    ## for instance
    ##     prev_dirst_array=( "workspace" "cse" "reference" "wrap-private-key")
    ##     dirs_array=( "workspace" "events" "docs" "release-notes")
    ##     first_discrepancy_level=2
    ## knowing that file_nesting_level will be determined by length of dirs_array
    ##   file_nesting_level=${#dirs_array[@]}
    ##
    ## And having 3 printing expressions
    ##
    ##
    ## # printing indentation tabs for a certain current_nesting_level
    ## for ((i=1 ; i<=${current_nesting_level}; i++)); do
    ##     for ((j=1; j<${i}; j++)); do
    ##         echo -ne "\t" >> ${MAIN_TOUPDATE}
    ##     done
    ## done
    ##
    ## # printing a linkto bulletpoint
    ## echo -ne "- & @${filename}@ ${paths_linkto[${file}]} &\n" >> ${MAIN_TOUPDATE}
    ##
    ## # printing a subdir bullet point
    ## echo -ne "- ${dirs_array[(${current_nesting_level})]}\n" >> ${MAIN_TOUPDATE}
    ##
    ## Iterating on the filelist
    ##     calculate first_discrepancy_level for each dirs_array in compare with prev_dirs_array
    ## Then for each file we will need to be iterating on each current_nesting_level
    ## # Provided that file_nesting_level=${#dirs_array[@]}
    ##      we iterate from first_discrepancy_level to file_nesting_level
    ##      setting each iteration as current_nesting_level
    ##      Starting from current_nesting_level=first_discrepancy_level
    ##      # printing indetation tabs for a certain current_nesting_level
    ##          if indentantion_nesting_level -eq to file_nesting_level
    ##          then we
    ##          # printing a linkto bulletpoint
    ##          otherwise
    ##          # printing a subdir bullet point
    ## Note: Base of index variables
    ##  first_discrepancy_level , base 0
    ##  file_nesting_level , base 1 , (converting to base 0)


    ## Compare current directory structure with the previous one to determine nesting
    for ((i=0; i<${#dirs_array[@]}; i++)); do
      if [ "${dirs_array[$i]}" != "${prev_dirs_array[$i]}" ]; then
        first_discrepancy_level=$i
        break
      fi
    done

    file_nesting_level=${#dirs_array[@]}
    file_nesting_level=$((file_nesting_level - 1))

    ## Iterating through nested levels and formatting output with indentation
    for ((current_nesting_level=first_discrepancy_level; current_nesting_level<=file_nesting_level; current_nesting_level++)); do

      for ((i=0; i<=current_nesting_level; i++)); do
        for ((j=0; j<i; j++)); do
          echo -ne "\t" >> "$MAIN_TOUPDATE"
        done
      done

      if [[ $current_nesting_level -eq $file_nesting_level ]]; then
        echo -ne "- & @${filename}@ ${paths_linkto[$file]} &\n" >> "$MAIN_TOUPDATE"
      else
        echo -ne "- ${dirs_array[($current_nesting_level)]}\n" >> "$MAIN_TOUPDATE"
      fi
    done

    prev_dirs_array=("${dirs_array[@]}")
  done

  echo "" >> "$MAIN_TOUPDATE"  ## Adding a line break

  ## Parsing and appending content, using Multi-rule
  for path in "${files_array[@]}"; do

    filename=$(basename "$path")
    echo "# ${filename} #" >> "$MAIN_TOUPDATE"
    echo "& ${paths_linkto[$path]} &" >> "$MAIN_TOUPDATE"
    echo "${paths_linkto[$path]}" | figlet >> "$MAIN_TOUPDATE"

    for content_parsing in "${content_parsing_array[@]}"; do
        # Create content parsing array
        content_dump=$(mktemp)
        found_selector=""


echo "content_parsing : ${content_parsing}" >&2 ## DEBUGGING

        eval "$content_parsing" < "$path" > "$content_dump"
        if [ 1 -lt $(stat -c %s "${content_dump}") ]; then

            echo "FOUND SELECTOR" >&2
            cat ${content_dump} >> "$MAIN_TOUPDATE"
            break
        fi


        # Default case for parsing , if none of the rules return a non-zero string
        if [ -z "$found_selector" ]; then
           cat ${path} | pandoc -f html -t plain > "$content_dump"
        fi

    done
  done
}


# @description Lists files or directories that are N levels below a specified subdirectory.
# This function searches for items (files or directories) at a specific depth relative to a given subdirectory.
# It supports optional recursion to include items in all subdirectories below the specified level.
#
# @arg $1 <items> The type of items to search for: "f" for files, "d" for directories.
# @arg $2 <level> The depth (number of levels) below the specified subdirectory to search.
# @arg $3 <directory> The name of the subdirectory to search within (e.g., "Global_Objects").
# @arg $4 <recursion> Whether to include items in all subdirectories below the specified level: "yes" or "no".
# @stdout Prints the paths of matching files or directories.
# @example getItemsNLevelDir "f" "1" "Global_Objects" "no"  # Lists direct children files of "Global_Objects"
# @example getItemsNLevelDir "d" "2" "Global_Objects" "yes" # Lists directories 2 levels below "Global_Objects" with recursion
function getItemsNLevelDir() {
    items=$1
    level=$2
    directory=$3
    recursion=$4
    regex="[A-Za-z0-9_/.-]*${directory}"

    for (( i=2; i<((${level} + 1)); i++ )); do
        echo ${i};
        regex+="\/[A-Za-z0-9_-]*"
    done

    if [ ${recursion} == 'yes' ]; then
        regex+="\/[A-Za-z0-9_/.-]*"
    else
        regex+="\/[A-Za-z0-9_.-]*"
    fi

    echo ${regex}
    find ./ -type ${items} -regex ${regex}
}

# @description Finds files with a given extension that have a sibling directory of the same name.
# This function searches for files with the specified extension in a directory and checks if there
# is a corresponding directory with the same name (excluding the extension). If such a directory
# exists, the file path is printed. The function processes subdirectories recursively.
#
# @arg $1 <dir> The directory to search in.
# @arg $2 <ext> The file extension to search for (e.g., "html").
# @stdout Prints the paths of files that have a sibling directory with the same name.
# @example find_same_name_sibling_directory '.' 'html'  # Finds .html files with sibling directories in the current directory
function find_same_name_sibling_directory() {
    local dir="$1"
    local ext="$2"
    # Find all .EXT files in the directory
    ext_files=$(find "$dir" -maxdepth 1 -type f -name "*.${ext}" -printf "%f\n")
    
    # Loop through EXT files
    for ext_file in $ext_files; do
        # Check if there's a corresponding directory with the same name
        if [ -d "${dir}/${ext_file%.*}" ]; then
            echo "${dir}/${ext_file}"
        fi
    done

    # Recursively process subdirectories
    subdirs=$(find "$dir" -mindepth 1 -maxdepth 1 -type d)
    for subdir in $subdirs; do
        find_same_name_sibling_directory "$subdir" "$ext"
    done
}

# @description Estimates the total plaintext size of documentation files by calculating the difference between HTML size and HTML boilerplate overhead.
# This function calculates the "HTML headroom" (boilerplate overhead) for a single HTML file and extrapolates it to all HTML files in the directory.
# It then subtracts the total headroom from the total HTML size to estimate the plaintext size of the documentation.
#
# The function outputs:
#   - HTML headroom for a single file.
#   - Total number of HTML files.
#   - Total headroom for all files.
#   - Total HTML size in bytes.
#   - Estimated plaintext size in bytes and megabytes.
#
# @arg $DOCU_PATH The base directory containing the downloaded HTML files.
# @stdout Prints the estimated plaintext size of the documentation in bytes and megabytes.
# @example estimate_docu_weight  # Estimates the plaintext size of documentation in ${DOCU_PATH}/downloaded
function estimate_docu_weight() {
    mapfile -t files_array < <(find "${DOCU_PATH}/downloaded" -type f -name "*.html" | sort -V)

    # Get plaintext size
    plaintext_size=$(cat "${files_array[0]}" | pup -i 0 --pre 'article div.devsite-article-body' | pandoc -f html -t plain --wrap=none | wc -c)

    # Get HTML file size
    html_size=$(stat -c%s "${files_array[0]}")

    html_headroom=$((${html_size} - ${plaintext_size}))

    echo "HTML headroom: ${html_headroom}"

    # Get the total number of HTML files
    num_files=$(find "${DOCU_PATH}/downloaded" -maxdepth 1 -type f -name "*.html" | wc -l)

    echo "Number of HTML files: ${num_files}"

    total_headroom=$((${html_headroom} * ${num_files}))
    echo "Total Headroom: ${total_headroom}"

    html_size=$(du -sb "${DOCU_PATH}/downloaded" | awk '{print $1}')
    echo "HTML size: ${html_size}"

    bytes_total_docu=$((${html_size} - ${total_headroom}))

    # Convert total plaintext bytes to megabytes
    bytes_total_docu_mb=$(echo "scale=2; $bytes_total_docu / 1048576" | bc)

    echo "Total documentation bytes (estimated plaintext): ${bytes_total_docu} bytes"
    echo "Total documentation size (estimated plaintext) in MB: ${bytes_total_docu_mb} MB"
}


# @description Splits documentation files into a specified number of parts of approximately equal size.
# This function calculates the size of each split and identifies the files where the splits should occur.
# It assumes that the documentation files are sorted alphabetically and will be processed in that order.
#
# The function outputs the filenames where each split occurs and the size of each split.
#
# @arg $1 <no_splits> The number of splits to create.
# @arg $2 <path> The directory containing the HTML files to split.
# @stdout Prints the split points (filenames) and the size of each split.
# @example get_split_files 3 "${DOCU_PATH}/downloaded"  # Splits files in ${DOCU_PATH}/downloaded into 3 parts
function get_split_files() {
    no_splits=$1
    path=$2

    html_size=$(du -sb "${path}/" | awk '{print $1}')

    mapfile -t files_array < <(find "${path}/" -type f -name "*.html" | sort -V)
    acumulated_size=0
    chunk_size=$((${html_size} / ${no_splits}))
    split_count=0

    for file in "${files_array[@]}"; do
        current_filesize=$(stat -c%s "${file}")
        acumulated_size=$((${acumulated_size} + ${current_filesize}))

        if [[ ${acumulated_size} -gt ${chunk_size} ]]; then
            split_count=$((${split_count} + 1))
            echo "Split no: ${split_count} split at file: ${file}"
            acumulated_size=0
            if [[ $((${split_count} + 1)) -eq ${no_splits} ]]; then
                break
            fi
        fi
    done
}

# @description Splits a subset of documentation files into a specified number of parts of approximately equal size.
# This function is similar to `get_split_files`, but it operates on a subset of files between `file_start` and `file_finnish`.
# It calculates the size of each split and identifies the files where the splits should occur.
# The function assumes that the documentation files are sorted alphabetically and will be processed in that order.
#
# The function outputs the filenames where each split occurs and the size of each split.
#
# @arg $1 <no_splits> The number of splits to create.
# @arg $2 <path> The directory containing the HTML files to split.
# @arg $3 <file_start> The starting file (inclusive) for the subset of files to process.
# @arg $4 <file_finnish> The ending file (inclusive) for the subset of files to process.
# @stdout Prints the split points (filenames) and the size of each split.
# @example get_split_files_partial 3 "${DOCU_PATH}/downloaded" "cloud.google.com-java-docs.html" "cloud.google.com-java-getting-started-session-handling-with-firestore.html"
function get_split_files_partial() {
    no_splits=$1
    path=$2
    file_start=$3
    file_finnish=$4

    html_size=$(du -sb "${path}/" | awk '{print $1}')

    mapfile -t files_array < <(
        find "${path}/" -type f -name "*.html" | sort -V | \
        sed -n "/$file_start/,/$file_finnish/p"
    )
    acumulated_size=0
    chunk_size=$((${html_size} / ${no_splits}))
    split_count=0

    for file in "${files_array[@]}"; do
        current_filesize=$(stat -c%s "${file}")
        acumulated_size=$((${acumulated_size} + ${current_filesize}))

        if [[ ${acumulated_size} -gt ${chunk_size} ]]; then
            split_count=$((${split_count} + 1))
            echo "Split no: ${split_count} split at file: ${file}"
            acumulated_size=0
            if [[ $((${split_count} + 1)) -eq ${no_splits} ]]; then
                break
            fi
        fi
    done
}

# @description Deletes files with duplicate headers in a directory structure.
# This function searches for files with the same header (extracted using the `.head-ltitle` CSS selector)
# and removes duplicates, keeping only one instance of each unique header.
# It is designed to work with a directory structure where files are organized in subdirectories.
#
# @arg $DOCU_PATH The base directory containing the files to process.
# @stdout Prints the paths of deleted files.
# @example delete_duplicate_header_files  # Removes duplicate files in ${DOCU_PATH}/downloaded/bookworm/
function delete_duplicate_header_files() {
    for dir in "$DOCU_PATH"/downloaded/bookworm/*/; do
        mapfile -td '' files < <(find "$dir" -type f -name '*' -print0)

        if (( "${#files[@]}" > 1 )); then
            for file in "${files[@]}"; do
                title=$(pup .head-ltitle < "$file")

                for fileb in "${files[@]}"; do
                    if [[ $file != "$fileb" ]]; then
                        if [[ $title == "$(pup .head-ltitle < "$fileb")" ]]; then
                            printf 'progname: Removing Duplicate: %s\n' "$fileb"
                            rm -f -- "$fileb"
                        fi
                    fi
                done
            done
        fi
    done
}

# @description A function to store codeblocks that are not used 
function unused_snippets() {
    rename -f "s/ /_/g" ${DOCU_PATH}/downloaded/**/*.*
    rename "s/\*/asterisk/g" ${DOCU_PATH}/downloaded/**/*.*

    # Removing non-exFAT compatible filename files
    #      not needed if index has --restrict-file-names=windows
    find ./ -type f -regex "\(.*\?.*\|.*\%.*\)" -exec rm {} \;



    ## Removing duplicates and other clutter
    find ${DOCU_PATH}/downloaded -not -path "${DOCU_PATH}/downloaded/bookworm/*" -delete
    find ${DOCU_PATH}/downloaded -type f -name 'index.html' -delete

    ## Removing other languages other than english
    find . -type f \( -name "*.html" ! -name "*en.html" \) -exec rm {} \;
    find ${DOCU_PATH}/downloaded -type f \( -name "*.html" ! -name "*en.html" \) -exec rm {} \;

}


## EOF EOF EOF Deprecated Functions 
## ----------------------------------------------------------------------------



## ----------------------------------------------------------------------------
# @section Spidering Stage Utilities
# @description Standard Spidering Subroutines implemented across different documentations

# @description Performs an inte
# @option -l <DOWNLOAD_LINK> Option Description
# @stdout Return Description
# @example standard_spider -l 
function standard_spider() {
    ## PARSING FUNCTION ARGUMENTS ------------------------
    local OPTIND=1
    # Initialize variables
    DOWNLOAD_LINK=""

    while getopts "l:" opt; do
        case "${opt}" in
            l) DOWNLOAD_LINK="${OPTARG}" ;;
            *)
                echo "Usage: standard_spider -l DOWNLOAD_LINK" >&2
                echo "Example: standard_spider -l DOWNLOAD_LINK" >&2
                return 1
                ;;
        esac
    done
    # Variable check
    if [[ -z "${DOWNLOAD_LINK}" ]]; then
        echo "Error: standard_spider requires -l DOWNLOAD_LINK" >&2
        return 1
    fi
    ## EOF EOF EOF PARSING FUNCTION ARGUMENTS -------------



    ## Create a <websiteLinks>.csv with the spidered links, 
    ##      for further processing with download_fromlist
    DOWNLOAD_LINK=${1}

    ntfs_filename=$(echo "${DOWNLOAD_LINK}" | sed 's/[<>:"\/\\|?*]/_/g')
    ## If there is an existing link-list, make a backup of it
    if [ -f "$CURRENT_DIR/../index-links/${ntfs_filename}.csv" ]; then
        echo "A previous link-list has been found, making a backup of the old-one" >&2
        cp "$CURRENT_DIR/../index-links/${ntfs_filename}.csv" "$CURRENT_DIR/../index-links/${ntfs_filename}-bk.csv"
    fi

    echo "Spidering a new link-list" >&2
    echo "This may take a while, and has to be done in one go" >&2
    echo "Consider doing this using a VPS ..." >&2

    wget \
    `## Basic Startup Options` \
      --execute robots=off \
    `## Loggin and Input File Options` \
      --force-html \
    `## Download Options` \
      --spider \
    `## Directory Options` \
      --no-directories \
    `## HTTP Options` \
      --user-agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36 Edg/91.0.864.59" \
    `## HTTPS Options` \
      --no-check-certificate \
      --https-only \
    `## Recursive Retrieval Options` \
      --recursive --level=inf \
      --delete-after \
    `## Recursive Accept/Reject Options` \
      --accept '*.html,*.htm' \
      --reject-regex '.*?hel=.*|.*?hl=.*|.*[%&=?].*|\\\"' \
      "${DOWNLOAD_LINK}" 2>&1  \
          | grep '^--' | awk '{ print $3 }' | awk '{ print $0 ",-1" }' | sort -u \
      > "$CURRENT_DIR/../index-links/${ntfs_filename}.csv"

      # Compressing the file
      bzip2 "$CURRENT_DIR/../index-links/${ntfs_filename}.csv"

}

##      --reject '*.jpg,*.svg,*.js,*.json,*.css,*.png,*.xml,*.txt,*.mp4,*.gif,*.webp,*.ico,*.woff,*.woff2,*.ttf,*.pdf,*.java,*.sql,*.jar,*.zip,*.GIF,*.PNG,*.svgz,*.webp,*.mp4,*.ico,*.gif,*.jpg,*.svg,*.js,*.json,*.css,*.png,*.xml,*.txt,*.webp,*.mp4,*.ico,*.gif,*.jpg,*.svg,*.js,*.json,*.css,*.png,*.xml,*.txt,*.tar,*.pcap,*.pcapng,*.lua,*.msi,*.exe,*.gz,*.csl,*.text,*.tex' \


## EOF EOF EOF Spidering Stage Utilities 
## ----------------------------------------------------------------------------

## ----------------------------------------------------------------------------
# @section Indexing Stage Utilities
# @description Standard Indexing Subroutines implemented across different documentations


# @description download_fromlist Function Description
# Out of a <websiteLinks>.csv created from standard_spider()
#      Download each link
#          - Updating the .csv with status code
#          - Download it to DOCU_PATH/downloaded
# @option -l <DOWNLOAD_LINK> Option Description
# @stdout Return Description
# @example download_fromlist -l 
function download_fromlist() {
    ## PARSING FUNCTION ARGUMENTS ------------------------
    local OPTIND=1
    # Initialize variables
    DOWNLOAD_LINK=""

    while getopts "l:" opt; do
        case "${opt}" in
            a) DOWNLOAD_LINK="${OPTARG}" ;;
            *)
                echo "Usage: download_fromlist -l DOWNLOAD_LINK" >&2
                echo "Example: download_fromlist -l DOWNLOAD_LINK" >&2
                return 1
                ;;
        esac
    done
    # Variable check
    if [[ -z "${DOWNLOAD_LINK}" ]]; then
        echo "Error: download_fromlist requires -l DOWNLOAD_LINK" >&2
        return 1
    fi
    ## EOF EOF EOF PARSING FUNCTION ARGUMENTS -------------



}


# @description 
# Out of a <websiteLinks>.csv created from standard_spider()
#      Download each link
#          - Updating the .csv with status code
#          - Download it to DOCU_PATH/downloaded
# @option -l <DOWNLOAD_LINK> 
# @option -w <WAIT> Seconds to wait per link
# @option -r <WAIT_RETRY> Seconds to wait per unsuccesful download
# @stdout Return Description
# @example download_fromlist_waitretry -l -w -r 
function download_fromlist_waitretry() {
    ## PARSING FUNCTION ARGUMENTS ------------------------
    local OPTIND=1
    # Initialize variables
    DOWNLOAD_LINK=""
    WAIT=""
    WAIT_RETRY=""

    while getopts "l:w:r:" opt; do
        case "${opt}" in
            l) DOWNLOAD_LINK="${OPTARG}" ;;
            w) WAIT="${OPTARG}" ;;
            r) WAIT_RETRY="${OPTARG}" ;;
            *)
                echo "Usage: download_fromlist_waitretry -l DOWNLOAD_LINK -w WAIT -r WAIT_RETRY" >&2
                echo "Example: download_fromlist_waitretry -l DOWNLOAD_LINK -w WAIT -r WAIT_RETRY" >&2
                return 1
                ;;
        esac
    done
    # Variable check
    if [[ -z "${DOWNLOAD_LINK}" || -z "${WAIT}" || -z "${WAIT_RETRY}" ]]; then
        echo "Error: download_fromlist_waitretry requires -l DOWNLOAD_LINK -w WAIT -r WAIT_RETRY" >&2
        return 1
    fi
    ## EOF EOF EOF PARSING FUNCTION ARGUMENTS -------------
    ntfs_filename=$(echo "${DOWNLOAD_LINK}" | sed 's/[<>:"\/\\|?*]/_/g')

    ## Create downloaded directory if it doesn't exists
    [ ! -d "${DOCU_PATH}/downloaded" ] && mkdir -p "${DOCU_PATH}/downloaded"

    ## Check if a local csv has been copied, if not copy the one from the repository 
    LOCAL_CSV_PATH="${DOCU_PATH}/${ntfs_filename}.csv"
    if [ ! -f ${DOCU_PATH}/${ntfs_filename}.csv ]; then
        echo "No previous local csv has been found" >&2
        echo "Starting the index from the scratch ..." >&2
        bunzip2 -kc "$CURRENT_DIR/../index-links/${ntfs_filename}.csv.bz2" > "${LOCAL_CSV_PATH}"
#        cp "$CURRENT_DIR/../index-links/${ntfs_filename}.csv" "${LOCAL_CSV_PATH}"
    else
        echo "Resuming a previous download ..." >&2
    fi


    ## We are iterating through the .csv line by line
    total_lines=$(wc -l < ${LOCAL_CSV_PATH} ) 
    row_no=1

    while [ "$row_no" -le "$total_lines" ]; do
        row=$(sed -n "${row_no}p" "${LOCAL_CSV_PATH}" )  # Extract the specific row number $row_no

        ## Parsing url and exit_status
        url=$(echo "$row" | cut -d',' -f1)
        exit_status=$(echo "$row" | cut -d',' -f2)

        
        ## If the file hasnt been attempted to download yet
        if [ "$exit_status" -ne 0 ] && [ "$exit_status" -ne 8  ] && [ "$exit_status" -ne 6  ]; then 
            url=$(echo "$row" | cut -d',' -f1)
            dirpath=$(echo "$url" | sed 's|https://||;s|/[^/]*$||')
            filename=$(basename "$url")

            ## If the file doesnt exist locally
            if [ ! -f ${DOCU_PATH}/downloaded/${dirpath}/${url} ]; then
                wget -nc --adjust-extension --directory-prefix=${DOCU_PATH}/downloaded/${dirpath} ${url} > /dev/null 2>&1
                wget_status=${?}
            else ## File was already downloaded,update the wget_status
                echo "Found the file locally, despite not been in the list : ${url}" >&2
                wget_status="0"
            fi

            if [ "$wget_status" -eq 8 ]; then
                echo "There was a server error"
                echo "Cooling it down and retrying after the wait_retry : ${WAIT_RETRY}"
                sleep ${WAIT_RETRY} 
            fi

            if [ "$wget_status" -eq 0 ]; then
                sleep ${WAIT}
            fi

            ## Update in-place the row of the file with the new wget_status
            row="${url},${wget_status}"

            echo "Updated row : ${row}" >&2
            echo "Completed ${row_no} out of ${total_lines}" >&2

            sed -i "${row_no}c\\${row}" "${LOCAL_CSV_PATH}"
        fi
            ((row_no++))
    done



}


# @description standard_index Function Description
# @option -l <DOWNLOAD_LINK> Option Description
# @stdout Return Description
# @example standard_index -l 
function standard_index() {
    ## PARSING FUNCTION ARGUMENTS ------------------------
    local OPTIND=1
    # Initialize variables
    DOWNLOAD_LINK=""

    while getopts "l:" opt; do
        case "${opt}" in
            l) DOWNLOAD_LINK="${OPTARG}" ;;
            *)
                echo "Usage: standard_index -l DOWNLOAD_LINK" >&2
                echo "Example: standard_index -l DOWNLOAD_LINK" >&2
                return 1
                ;;
        esac
    done
    # Variable check
    if [[ -z "${DOWNLOAD_LINK}" ]]; then
        echo "Error: standard_index requires -l DOWNLOAD_LINK" >&2
        return 1
    fi
    ## EOF EOF EOF PARSING FUNCTION ARGUMENTS -------------

        wget \
        `##tBasic Startup Options` \
          --execute robots=off \
        `## Loggin and Input File Options` \
        `## Download Options` \
          --timestamping \
        `## Directory Options` \
          --directory-prefix=${DOCU_PATH}/downloaded \
          -nH \
        `## HTTP Options` \
          --user-agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36 Edg/91.0.864.59" \
          --adjust-extension \
        `## HTTPS Options` \
          --no-check-certificate \
        `## Recursive Retrieval Options` \
          --recursive --level=inf \
        `## Recursive Accept/Reject Options` \
          --no-parent \
          --reject '*.jpg,*.svg,*.js,*.json,*.css,*.png,*.xml,*.txt,*.mp4,*.gif,*.webp,*.ico,*.woff,*.woff2,*.ttf,*.pdf,*.java,*.sql,*.jar,*.zip,*.GIF,*.PNG,*.svgz,*.webp,*.mp4,*.ico,*.gif,*.jpg,*.svg,*.js,*.json,*.css,*.png,*.xml,*.txt,*.webp,*.mp4,*.ico,*.gif,*.jpg,*.svg,*.js,*.json,*.css,*.png,*.xml,*.txt' \
          --reject-regex '.*?hel=.*|.*?hl=.*|.*[%&=?].*|\\\"' \
          --page-requisites \
          ${DOWNLOAD_LINK}

}


## EOF EOF EOF Indexing Stage Utilities 
## ----------------------------------------------------------------------------




## ----------------------------------------------------------------------------
# @section Arranging Stage Utilities
# @description Standard Arraging Subroutines shared across different documentations



## EOF EOF EOF Arranging Stage Utilities 
## ----------------------------------------------------------------------------


## ----------------------------------------------------------------------------
# @section Parsing Stage Utilities
# @description Standard Parsing Subroutines shared across different documentations

prepocess_LinkSources() {
    ## PARSING FUNCTION ARGUMENTS ------------------------
    ## ---------------------------------------------------
    local OPTIND=1
    # Initialize variables
    DOCU_NAME=""
    BUID_CSV=""

    while getopts "a:z:z:z:z:z:z:z:z:z:z:z:z:z:" opt; do
        case "${opt}" in
            a) DOCU_NAME="${OPTARG}" ;;
            a) BUID_CSV="${OPTARG}" ;;
            *)
                echo "Usage: prepocess_LinkSources -a DOCU_NAME -z BUID_CSV" >&2
                echo "Example: prepocess_LinkSources -a DOCU_NAME -z BUID_CSV" >&2
                return 1
                ;;
        esac
    done
    # Variable check
    if [[ -z "${DOCU_NAME}" || -z "${BUID_CSV}" ]]; then
        echo "Error: prepocess_LinkSources requires -a DOCU_NAME -z BUID_CSV" >&2
        return 1
    fi
    ## PARSING FUNCTION ARGUMENTS ------------------------
    ## EOF EOF EOF EOF------------------------------------



}


# @description Write the vim-dan generic header
function write_header() {
    # Header of docu    
    echo "vim-dan" | figlet -f univers > ${MAIN_TOUPDATE}
    echo ${DOCU_NAME} | figlet >> ${MAIN_TOUPDATE}
    echo "Documentation indexed from :" >> ${MAIN_TOUPDATE}
    for DOWNLOAD_LINK in "${DOWNLOAD_LINKS[@]}"; do
        echo " - ${DOWNLOAD_LINK} " >> ${MAIN_TOUPDATE}
    done 
    echo "Last parsed on : $(date)" >> ${MAIN_TOUPDATE}
}

# @description write_html_docu_multirule Function Description
# Similar as write_docu_multirule() but simplified, just to input html tags such as
#   write_html_docu_multirule -f "head title" -b "div.guide-content" -b "body"
#  For understanding the code , there are not comments but just the added functionality ones
# from write_docu_multirule()
# It needs ${DOCU_NAME} to be setted
# It needs ${DOCU_PATH} to be setted
# @option -f <TITLE_SELECTOR>... Specify a title html selector (repeat -f to specify multiple posible selectors)
# @option -b <BODY_SELECTOR>... Specify a body html selector (repeat -b to specify multiple posible selectors)
# @option -L [PANDOC_FILTER] Add a pandoc filter (optional)
# @option -fm [FILE_MANGLING] Add some string manipulation commands to filter with the whole document afterwards (optional)
# @option -c [WRAP_COLUMNS] Inidicate no of columns for the output to be wrapped to (default non-wrap)
# @example 
#   write_html_docu_multirule -f "head title" -b "div.guide-content" -b "body" -cp -fm "tail -n +6 
function write_html_docu_multirule() {
    ## PARSING FUNCTION ARGUMENTS ------------------------
      local title_parsing_array=()
      local content_parsing_array=()
      local pandoc_filters_array=()

      ## Parse options for title and body rules
      while [[ "$#" -gt 0 ]]; do
        case "$1" in
          -f)
            if [[ -n "$2" ]]; then
              title_parsing_array+=("$2")
              shift 2
            else
              echo "Error: Missing argument for -f" >&2
              return 1
            fi
            ;;
          -b)
            if [[ -n "$2" ]]; then
              content_parsing_array+=("$2")
              shift 2
            else
              echo "Error: Missing argument for -b" >&2
              return 1
            fi
            ;;
          -L)
            if [[ -n "$2" ]]; then
              pandoc_filters_array+=("$2")
              shift 2
            fi
            ;;
          -cp)
            codeblock_prompt=true
            shift
            ;;
          -li)
            links_internal=true
            shift
            ;;
          -la)
                if [[ -n "$2" ]]; then
                    links_anchor="$2"
                    shift 2
                else
                    echo "Error: Missing argument for -fm" >&2
                    return 1
                fi
            ;;
          -n)
                if [[ -n "$2" ]]; then
                    DOCU_NAME="$2"
                    shift 2
                else
                    echo "Error: Missing argument for -n" >&2
                    return 1
                fi
            ;;
          -p)
                if [[ -n "$2" ]]; then
                    DOCU_PATH="$2"
                    shift 2
                else
                    echo "Error: Missing argument for -p" >&2
                    return 1
                fi
            ;;
          -fm)
                if [[ -n "$2" ]]; then
                    file_mangling="$2"
                    shift 2
                else
                    echo "Error: Missing argument for -fm" >&2
                    return 1
                fi
            ;;
            -c)  # New option for columns
                if [[ -n "$2" ]]; then
                    wrap_columns="$2"
                    shift 2
                else
                    echo "Error: Missing argument for -c" >&2
                    return 1
                fi
            ;;
          *)
            echo "Unknown option: $1" >&2
            return 1
            ;;
        esac
      done
    ## EOF EOF EOF PARSING FUNCTION ARGUMENTS -------------

    ## IMPLICIT DEPENDENCY CHECKS -------------------------
    if [[ -z "$DOCU_NAME" || -z "$DOCU_PATH"  ]]; then
        echo "Error: Missing a implicit dependency, {DOCU_NAME} or {DOCU_PATH}" >&2
        return 1
    fi
    ## ----------------------------------------------------

  mapfile -t files_array < <(find "${DOCU_PATH}/downloaded" -type f -name "*.html" | sort -V)

    ## Prepocessing variables
    if [[ -n "$file_mangling" ]]; then
        file_mangling+="| sed -E ':a; N; s/\n(%\\$.*?%\\$)/ \1/'"
    else
        file_mangling="sed -E ':a; N; s/\n(%\\$.*?%\\$)/ \1/'"
    fi



  ## First create the title array
  title_array=()
  for file in "${files_array[@]}"; do

    # (Multi-rule) Parsing functions, add as many as needed
    found_selector=""
    for title_parsing in "${title_parsing_array[@]}"; do
      # Formulating command
      cmd="pup -i 0 --pre '${title_parsing}' | pandoc -f html -t plain --wrap=none"

      title=$(eval "$cmd" < "$file" | sed -e ':a;N;$!ba;s/\n/ /g' | sed -e 's/ ¶//g' | sed -e 's/¶//g')
      if [ -n "$title" ]; then
        found_selector=true
        break
      fi
    done

    # Default case for parsing, if none of the rules return a non-zero string
    if [ -z "$found_selector" ]; then
      title=$(basename "$file" | sed 's/\.html$//' )
    fi

    # Append the title to the title_array
    title_array+=("$title")
  done

  ## Creating an associative array to map titles to file paths
  declare -A paths_linkto

  # Iterate through the indices of 'files_array'
  for index in "${!files_array[@]}"; do
    file="${files_array[$index]}"
    title="${title_array[$index]}"
    paths_linkto["$file"]="$title"
  done

  # Create the linkFrom index header using figlet
  echo "index" | figlet >> "$MAIN_TOUPDATE"
  prev_dirs_array=()

  ## Splitting directories from filenames and building nested lists

  file_no=1
  for file in "${files_array[@]}"; do

    filename=$(echo ".${file#${DOCU_PATH}/downloaded}" | sed 's|^\.||')
    unset dirs_array
    remaining_file=$(echo "$filename" | sed 's|^/||')

    for ((i=0; i<25; i++)); do
        if [[ "$remaining_file" =~ ([^/]+)/(.+) ]]; then
        dirs_array[i]="${BASH_REMATCH[1]}"
        remaining_file="${BASH_REMATCH[2]}"
      else
        dirs_array[i]="$remaining_file"
        break
      fi
    done

    ## Compare current directory structure with the previous one to determine nesting
    for ((i=0; i<${#dirs_array[@]}; i++)); do
      if [ "${dirs_array[$i]}" != "${prev_dirs_array[$i]}" ]; then
        first_discrepancy_level=$i
        break
      fi
    done

    ## dirsy_array will have length 1 when file_nesting_level=0 etc.. , decrease it
    file_nesting_level=$((${#dirs_array[@]} - 1))

    ## Iterating through nested levels and formatting output with indentation
    for ((current_nesting_level=first_discrepancy_level; current_nesting_level<=file_nesting_level; current_nesting_level++)); do

        for ((i=0; i<current_nesting_level; i++)); do
          echo -ne "\t" >> "$MAIN_TOUPDATE"
        done

      if [[ $current_nesting_level -eq $file_nesting_level ]]; then
         echo "<L=$(decimal_to_alphanumeric ${file_no})>${paths_linkto[$file]}</L>" >> "$MAIN_TOUPDATE"
#        echo -ne "- &${paths_linkto[$file]}@${filename}@&\n" >> "$MAIN_TOUPDATE"
      else
        echo -ne "- ${dirs_array[($current_nesting_level)]}\n" >> "$MAIN_TOUPDATE"
      fi
    done

    prev_dirs_array=("${dirs_array[@]}")
  file_no=$((file_no + 1))
  done



  echo "" >> "$MAIN_TOUPDATE"  ## Adding a line break



    ## Initiating variables for codeblock_prompt
    if [[ "$codeblock_prompt" == true ]]; then
        scr_choice_1=$(mktemp)
        scr_choice_2=$(mktemp)
        scr_choice_3=$(mktemp)
        csv_path="$CURRENT_DIR/../ext-csv/${DOCU_NAME}.csv"
        [ ! -f "$csv_path" ] && mkdir -p "$(dirname "$csv_path")"
        scr_global_current_cb=$(mktemp)
        scr_csv_iterator_needs_catchup=$(mktemp)
    fi

  file_no=1

  ## Parsing and appending content, using Multi-rule
  for path in "${files_array[@]}"; do



    filename=$(echo ".${path#${DOCU_PATH}/downloaded}" | sed 's|^\.||')
    set -x
    echo "<B=$(decimal_to_alphanumeric ${file_no})>${paths_linkto[$path]}" >> "$MAIN_TOUPDATE"
    set +x
#    echo "# ${filename} #" >> "$MAIN_TOUPDATE"
    echo "& ${paths_linkto[$path]} &" >> "$MAIN_TOUPDATE"
    echo "${paths_linkto[$path]}" | figlet >> "$MAIN_TOUPDATE"

    found_selector=""
    for content_parsing in "${content_parsing_array[@]}"; do

        # Create content parsing array
        content_dump=$(mktemp)




        # Formulating command
    
        # Initial cmd
        cmd="sed 's/role=\"main\"//g' | pup -i 0 --pre '${content_parsing}' | pandoc -f html -t plain -o ${content_dump} -V file_processed=\"${filename}\" -V file_no=\"${file_no}\""

        [[ -n "$wrap_columns" ]] && cmd+=" --columns=${wrap_columns}"


        ## Adding codeblock_prompt
        [[ "$codeblock_prompt" == true ]] && cmd+=" -V scr_choice_1=${scr_choice_1} -V scr_choice_2=${scr_choice_2} -V scr_choice_3=${scr_choice_3} -V csv_path=${csv_path} -V scr_global_current_cb=${scr_global_current_cb} -V scr_csv_iterator_needs_catchup=${scr_csv_iterator_needs_catchup} -L $(realpath ${CURRENT_DIR}/../pandoc-filters/codeblock-prompt.lua)"

        ## Adding links_anchor
        if [[ -n "$links_anchor" ]]; then
            cmd+=" -V links_anchor=\"true\" -V objects_toparse=\"${links_anchor}\" -L $(realpath ${CURRENT_DIR}/../pandoc-filters/links-anchor.lua)"

        fi

        ## Adding links_internal filter
        [[ "$links_internal" == true ]] && cmd+=" -V docu_path=${DOCU_PATH} -L $(realpath ${CURRENT_DIR}/../pandoc-filters/links-internal.lua)"

        
        ## Adding the custom pandoc filters
        if [[ ${#pandoc_filters_array[@]} -gt 0 ]]; then
            for pandoc_filter in "${pandoc_filters_array[@]}"; do
#                cmd+="-L \"$(realpath \"${CURRENT_DIR}/../pandoc-filters/${pandoc_filter}\")\""
                cmd+=" -L ${pandoc_filter}"
            done
        fi

        eval "$cmd" < "$path"

        if [ 1 -lt $(stat -c %s "${content_dump}") ]; then
            if [[ -n "$file_mangling" ]]; then
                eval "cat ${content_dump} | ${file_mangling} >> ${MAIN_TOUPDATE}"
            else
                cat ${content_dump} >> "${MAIN_TOUPDATE}"
            fi
            found_selector=true
            break
        fi



    done

    # Default case for parsing , if none of the rules return a non-zero string
    if [ -z "$found_selector" ]; then
        cmd="pandoc -f html -t plain"
        [[ -n "$wrap_columns" ]] && cmd+=" --columns=${wrap_columns}"
        [[ -n "$file_mangling" ]] && cmd+=" | ${file_mangling}"
        eval "cat ${path} | ${cmd} >> ${MAIN_TOUPDATE}"
    fi


    echo "</B>" >> "${MAIN_TOUPDATE}"                        ## ADDING A LINE BREAK

    printf '%s\n' "$(for ((i=1; i<=${wrap_columns:-80}; i++)); do printf '='; done)" >> "${MAIN_TOUPDATE}"

    rm "$content_dump"

    file_no=$((file_no + 1))
  done



}


# @description Create a dan modeline for the different syntaxes that show in the file
function write_ext_modeline() {
    ## Gathering different syntax blocks in the file
    mapfile -t ext_array < <(grep -o -P '^```(\w+)$' ${MAIN_TOUPDATE} | sed 's/^```//' | sort | uniq -c | sort -nr | awk '{print $2}' )

    ext_string=$(IFS=, ; echo "${ext_array[*]}")

    ## Preparing the vim modeline
    modeline="&@ g:dan_ext_list = \"$ext_string\" @&"

    ## Prepending the modeline to the document
    sed -i "11i ${modeline}" ${MAIN_TOUPDATE}
}

## EOF EOF EOF Parsing Stage Utilities 
## ----------------------------------------------------------------------------
